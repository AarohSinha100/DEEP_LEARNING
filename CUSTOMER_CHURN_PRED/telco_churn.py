# -*- coding: utf-8 -*-
"""TELCO_CHURN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LYsz_nvelyS6-v3ZSjqezUkyYlkTrHRR
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf

from google.colab import files

uploaded = files.upload()

import io

churn_dataset = pd.read_csv(io.BytesIO(uploaded["WA_Fn-UseC_-Telco-Customer-Churn.csv"]))
churn_dataset.head()

churn_dataset.drop("customerID",axis=1,inplace=True)

print(f"The shape of the dataset is {churn_dataset.shape}")
print(f"The length of the churn dataset is {len(churn_dataset)}")

"""#### We have 20 columns - dividing this into 2 parts of 10 columns each to get better view of data"""

churn_part_1 = churn_dataset[churn_dataset.columns[:10]]
churn_part_1.head()

churn_dataset.dropna(axis=0,inplace=True)

churn_part_2 = churn_dataset[churn_dataset.columns[10:]]
churn_part_2.head()

churn_dataset.info()

churn_dataset.isnull().sum()

print(f"The shape of the dataset is {churn_dataset.shape}")
print(f"The length of the churn dataset is {len(churn_dataset)}")

def gender(value):
  return 0 if value == "Female" else 1

churn_dataset["gender"] = churn_dataset["gender"].apply(lambda x:gender(x))

def partner(value):
  return 0 if value=="No" else 1

churn_dataset["Partner"] = churn_dataset["Partner"].apply(lambda x:partner(x))

def Dependents(value):
  return 0 if value=="No" else 1

churn_dataset["Dependents"] = churn_dataset["Partner"].apply(lambda x:Dependents(x))

churn_dataset["PhoneService"].value_counts()

def PhoneService(value):
  return 0 if value == "No" else 1

churn_dataset["PhoneService"] = churn_dataset["PhoneService"].apply(lambda x: PhoneService(x))

churn_dataset["MultipleLines"].value_counts()

def MultipleLines(value):
  if value=="No":
    return 0
  elif value=="Yes":
    return 1
  else:
    return 2

churn_dataset["MultipleLines"] = churn_dataset["MultipleLines"].apply(lambda x: MultipleLines(x))

def OnlineSecurity(value):
  if value=="No":
    return 0
  else:
    return 1
  
churn_dataset["OnlineSecurity"] = churn_dataset["OnlineSecurity"].apply(lambda x: OnlineSecurity(x))

def OnlineBackup(value):
  if value=="No":
    return 0
  else:
    return 1
  
churn_dataset["OnlineBackup"] = churn_dataset["OnlineBackup"].apply(lambda x: OnlineBackup(x))

churn_dataset["InternetService"].value_counts()

internet_service = pd.get_dummies(churn_dataset["InternetService"])
internet_service.drop("No",axis=1,inplace=True)
internet_service.head()

#churn_dataset.drop("InternetService",axis=1,inplace=True) ->Done
churn_dataset = pd.concat([churn_dataset, internet_service])
churn_dataset[churn_dataset.columns[:10]].head()

churn_part_2.head()

def DeviceProtection(value):
  return 0 if value == "No" else 1

churn_dataset["DeviceProtection"] = churn_dataset["DeviceProtection"].apply(lambda x:DeviceProtection(x))

def TechSupport(value):
  return 0 if value == "No" else 1

churn_dataset["TechSupport"] = churn_dataset["TechSupport"].apply(lambda x: TechSupport(x))

def StreamingTV(value):
  return 0 if value == "No" else 1

churn_dataset["StreamingTV"] = churn_dataset["StreamingTV"].apply(lambda x: StreamingTV(x))

def StreamingMovies(value):
  return 0 if value == "No" else 1

churn_dataset["StreamingMovies"] = churn_dataset["StreamingMovies"].apply(lambda x: StreamingMovies(x))

def PaperlessBilling(value):
  return 0 if value == "No" else 1

churn_dataset["PaperlessBilling"] = churn_dataset["PaperlessBilling"].apply(lambda x: PaperlessBilling(x))

churn_dataset

churn_dataset[churn_dataset.columns[10:]]

churn_dataset.head()

len(churn_dataset)

churn_dataset.isnull().sum()

churn_dataset["Contract"].value_counts()

contract = pd.get_dummies(churn_dataset["Contract"])
contract.head()

churn_dataset[churn_dataset.columns[10:]]

c_1 = churn_dataset[churn_dataset.columns[:10]]

churn_dataset["gender"].value_counts()

c_1.head()

c_1.dropna(axis=0,inplace=True)
c_1.head()

c_1

churn_dataset["DSL"] = churn_dataset["DSL"].fillna(0)
churn_dataset["Fiber optic"] = churn_dataset["Fiber optic"].fillna(0)

churn_dataset[churn_dataset.columns[10:]]

c_1

churn_dataset.isnull().sum()

churn_dataset.dropna(axis=0,inplace=True)
churn_dataset.head(5)

len(churn_dataset)

churn_part_2 = churn_dataset[churn_dataset.columns[11:]]
churn_part_1 = churn_dataset[churn_dataset.columns[:11]]

churn_part_2

churn_dataset["Contract"].value_counts()

churn_dataset[churn_dataset.columns[11:]]

churn_part_2

churn_dataset["Contract"].isnull().sum()

contract = pd.get_dummies(churn_dataset["Contract"])

contract.head()

len(contract)

churn_dataset[churn_dataset.columns[11:]]

one_year = []
two_year = []
for i in range(len(churn_dataset)):
  if churn_dataset["Contract"].iloc[i] == "One year":
    one_year.append(1)
    two_year.append(0)
  elif churn_dataset["Contract"].iloc[i] == "Two year":
    two_year.append(1)
    one_year.append(0)
  else:
    two_year.append(0)
    one_year.append(0)

one_year = pd.DataFrame(one_year, columns=["one_year"])
two_year = pd.DataFrame(two_year, columns=["two_year"])
contract = pd.concat([one_year, two_year],axis=1)

contract.head()

contract.isnull().sum()

len(contract)

churn_dataset = pd.concat([churn_dataset, contract],axis=1)
churn_dataset[churn_dataset.columns[11:]]

churn_dataset.drop("Contract",axis=1,inplace=True)

churn_dataset["PaymentMethod"].value_counts()

payment = pd.get_dummies(churn_dataset["PaymentMethod"])
payment.head()

churn_dataset = pd.concat([churn_dataset, payment],axis=1)

churn_dataset[churn_dataset.columns[11:]]

churn_dataset.drop("InternetService",axis=1,inplace=True)

churn_part_1

churn_dataset.drop("PaymentMethod",axis=1,inplace=True)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

tenure_s = churn_dataset["tenure"]
tenure_s.head()

churn_dataset["tenure"].max()

def MinMaxScaler(value,column):
  value = (value - churn_dataset[column].min()) / (churn_dataset[column].max() - churn_dataset[column].min())
  return value

churn_dataset["tenure"] = churn_dataset["tenure"].apply(lambda x: MinMaxScaler(x, "tenure"))
churn_dataset.drop("tenure_scaled",axis=1,inplace=True)

churn_dataset["MonthlyCharges"] = churn_dataset["MonthlyCharges"].apply(lambda x: MinMaxScaler(x, "MonthlyCharges"))

churn_dataset.drop("TotalCharges",axis=1,inplace=True)

churn_dataset

churn_dataset.info()

def churn(value):
  return 0 if value=="No" else 1

churn_dataset["Churn"] = churn_dataset["Churn"].apply(lambda x: churn(x))

plt.figure(figsize=(30,18))
sns.heatmap(churn_dataset.corr(),annot=True)

"""#### Train Test Split"""

from sklearn.model_selection import train_test_split

X = churn_dataset.drop("Churn",axis=1)
y = churn_dataset["Churn"]

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=42)

"""# Logistic Regression Model - 80.78%"""

from sklearn.linear_model import LogisticRegression

log_model = LogisticRegression()
log_model.fit(X_train, y_train)

log_preds = log_model.predict(X_test)

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

print(confusion_matrix(y_test, log_preds))
print(classification_report(y_test, log_preds))
print(accuracy_score(y_test, log_preds))

"""#Deep Learning Model"""

#Set the random seed
tf.random.set_seed(42)

#Make the model
model_1 = tf.keras.Sequential([
    tf.keras.layers.Dense(100,activation="sigmoid"),
    tf.keras.layers.Dense(10,activation="sigmoid"),
    tf.keras.layers.Dense(1,activation="sigmoid")
])

#Compile the model
model_1.compile(loss=tf.keras.losses.binary_crossentropy,
                optimizer=tf.keras.optimizers.Adam(),
                metrics=["accuracy"])

#Fit and run
history_1 = model_1.fit(X_train, y_train, epochs=100)

"""### ACCURACY OF MODEL_1 = 80.37"""

#Model 2
tf.random.set_seed(42)

model_2 = tf.keras.Sequential([
    tf.keras.layers.Dense(100,activation="sigmoid"),
    tf.keras.layers.Dense(50,activation="sigmoid"),
    tf.keras.layers.Dense(10,activation="sigmoid"),
    tf.keras.layers.Dense(1,activation="sigmoid")
])

model_2.compile(loss=tf.keras.losses.binary_crossentropy,
                optimizer=tf.keras.optimizers.Adam(lr='0.001'),
                metrics=["accuracy"])

history_2 = model_2.fit(X_train, y_train, epochs=200)

model_2.evaluate(X_test, y_test)

